{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始学习自然语言处理\n",
    "\n",
    "# 1. 了解一下什么是自然语言处理\n",
    "阅读一下 paddle 官网的文档: [自然语言处理：自然语言处理综述](https://www.paddlepaddle.org.cn/tutorials/projectdetail/3518060)\n",
    "\n",
    "一个重要的信息是： 语言是一种文本信息，而从计算角度，计算机无法直接处理这些文本。计算机是计算的机器，现有的计算机都以浮点数为输入和输出，擅长执行加减乘除类计算。自然语言本身并不是浮点数，计算机为了能存储和显示自然语言，需要把自然语言中的字符转换为一个固定长度（或者变长）的二进制编码：UTF-8。\n",
    "\n",
    "但是这个编码本身不是数字，对这个编码的计算往往不具备数学和物理含义。例如：把“法国”和“首都”放在一起，大多数人首先联想到的内容是“巴黎”。但是如果我们使用“法国”和“首都”的UTF-8编码去做加减乘除等运算，是无法轻易获取到“巴黎”的UTF-8编码，甚至无法获得一个有效的UTF-8编码。因此，如何让计算机可以有效地计算自然语言，是计算机科学家和工程师面临的巨大挑战。\n",
    "\n",
    "\n",
    "# 2. 词嵌入/词向量（Word Embedding）\n",
    "正如 1 中所说，如何把语言转换为“有意义的”浮点数，是一个挑战。Word Embedding 就是一种把词变成向量的方法（把词转换为N维空间的一个点），方便后续使用。\n",
    "\n",
    "首先的一点是，“词是什么？”。或者说词是编码的最小单位。 汉字的数目是有限的，为什么不直接把每个汉字进行编码。而词的数目可能会不断的增加，不通的时代，词甚至有不一样的意思，而且我们总会创建一些新词。\n",
    "\n",
    "比如一句话： “我爱人工智能”，如果以词来切分，就可以切分为： “我“， ”爱“， ”人工智能“\n",
    "\n",
    "如果以每个汉字来切分，“我“， ”爱“， ”人”， “工” ，“智”， “能“。\n",
    "\n",
    "很显然，从人类理解的角度出发，按词切分显得更合理（如果你硬要按汉字切分，当然也可以，但是古先圣贤的经验告诉我们，按词分是更好的）。\n",
    "\n",
    "在这之前，先想一下有哪些方式呢，如果把每一个词字转换为一个 one-hot 向量，这是一种最简单且可行的方式：比如假设词有 1000 个(实际数目远远不止)，\n",
    "\n",
    "那么第一个词对应的向量为: 1, 0, 0, 0, 0 .... \n",
    "\n",
    "第二个词为:             0, 1, 0, 0, 0, ...\n",
    "\n",
    "第三个词为:             0, 0, 1, 0, 0, ...\n",
    "\n",
    "但是存在的问题就是，词之间的联系却没有了，比如 “香蕉”，“橘子”这两个词，似乎是有一定联系的，但是这种词向量使得这 1000 个词之间的距离都是一样的。难以体现之间的联系（one hot 之间正交），因此我们可能需要在这 one-hot encoding 的基础上再做一些事情（乘以 Embedding 矩阵）。\n",
    "\n",
    "大部分词向量模型都需要回答两个问题：\n",
    "\n",
    "**如何把词转换为向量?**\n",
    "自然语言单词是离散信号，比如“香蕉”，“橘子”，“水果”在我们看来就是3个离散的词。如何把每个离散的单词转换为一个向量？\n",
    "\n",
    "**如何让向量具有语义信息?**\n",
    "比如，我们知道在很多情况下，“香蕉”和“橘子”更加相似，而“香蕉”和“句子”就没有那么相似，同时“香蕉”和“食物”、“水果”的相似程度可能介于“橘子”和“句子”之间。\n",
    "\n",
    "所以一个好的 Word Embedding 意义重大，在很大程度上决定了 nlp 任务的成败。\n",
    "\n",
    "阅读 paddle 官网文档： [自然语言处理：词向量 Word Embedding](https://www.paddlepaddle.org.cn/tutorials/projectdetail/3578658)\n",
    "\n",
    "可以在线执行官网的代码，或者在自己的环境中执行，参考 [Word Embeding](./Word%20Embeding.ipynb)\n",
    "\n",
    "这里面介绍了使用 CBOW 和 Skip-gram 的算法训练得到了一个词向量模型。如有兴趣可以跟着流程走一遍，实际上我们只需要使用古先圣贤们训练好的词向量模型就行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "使用 PaddleNLP 已预置多个公开的预训练的中文 Embedding\n",
    "注意这里使用的是 TokenEmbedding 而不是上面一直说的 WordEmbedding \n",
    "\n",
    "Token 是更专业的术语，Token可以是 Word（词），也可以是单个的字。nlp任务里的 Token 一般都是 Word，故在后面的描述中可以认为 Token == Word \n",
    "'''\n",
    "\n",
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "import paddle\n",
    "\n",
    "# 初始化TokenEmbedding， 预训练embedding未下载时会自动下载并加载数据\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")\n",
    "\n",
    "# 查看token_embedding详情\n",
    "print(token_embedding)\n",
    "\n",
    "'''\n",
    "可以看出，token_embedding 的shape 是[635965, 300]，即一共有 635965 个词，包括UNK，每个词由 300 纬的向量组成。\n",
    "\n",
    "从源码实现上来看， 继承关系为： TokenEmbedding --> nn.Embedding --> nn.Layer；是paddle中一个标准的 layer。\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "TokenEmbedding 暴露的主要 API 为：\n",
    "search， get_idx_from_word， get_idx_list_from_words，cosine_sim\n",
    "'''\n",
    "\n",
    "# 使用 search() 获得指定词汇的词向量。\n",
    "embedding_tensor = token_embedding.search(\"中国\")\n",
    "# print(embedding_tensor)\n",
    "\n",
    "# 也可以通过这种方式：先获取 token 对应的 idx，再根据 idx 获得 token_embedding\n",
    "idx = token_embedding.get_idx_from_word(\"中国\")\n",
    "embedding_tensor = token_embedding(paddle.to_tensor(idx))\n",
    "print(type(embedding_tensor))\n",
    "print(embedding_tensor.shape)\n",
    "# print(embedding_tensor)\n",
    "\n",
    "# embedding 多个词向量\n",
    "idxs = []\n",
    "idxs.append(token_embedding.get_idx_from_word(\"中国\"))\n",
    "idxs.append(token_embedding.get_idx_from_word(\"北京\"))\n",
    "embedding_tensors = token_embedding(paddle.to_tensor(idxs))\n",
    "print(type(embedding_tensors))\n",
    "print(embedding_tensors.shape)\n",
    "\n",
    "\n",
    "# cosine_sim() 计算词向量间余弦相似度，语义相近的词语余弦相似度更高，说明预训练好的词向量空间有很好的语义表示能力\n",
    "score1 = token_embedding.cosine_sim(\"女孩\", \"女人\")\n",
    "score2 = token_embedding.cosine_sim(\"女孩\", \"书籍\")\n",
    "print('score1:', score1)\n",
    "print('score2:', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 句向量（Sentence Embedding）\n",
    "有了词向量，如何得到句向量呢？\n",
    "\n",
    "此时我们可以使用词袋模型（Bag of Words，简称BoW）计算句子的语义向量。\n",
    "\n",
    "**首先**，将两个句子分别进行切词，并在 TokenEmbedding 中查找相应的单词词向量（word embdding）。\n",
    "\n",
    "**然后**，根据词袋模型，将句子的 word embedding 叠加作为句子向量（sentence embedding）。\n",
    "\n",
    "切词可以使用 jieba 第三方库，参考[jieba分词工具](./jieba%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7.ipynb)\n",
    "\n",
    "也可以使用 JiebaTokenizer, 这是对 jieba 的简单封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '来自', '清华', '大学']\n",
      "Tensor(shape=[4, 300], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[-0.07075300, -0.00312400,  0.18455200, ...,  0.04748000,\n",
      "         -0.22542900, -0.42837900],\n",
      "        [ 0.13855401,  0.08321500, -0.21475700, ..., -0.07017700,\n",
      "         -0.20189700, -0.03826100],\n",
      "        [ 0.28273299,  0.00419400, -0.36795101, ...,  0.02538100,\n",
      "          0.09789900, -0.19698900],\n",
      "        [ 0.06500300,  0.19419700, -0.33714399, ..., -0.09627100,\n",
      "          0.06636300, -0.22596300]])\n",
      "Tensor(shape=[1, 300], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.41553700,  0.27848199, -0.73529994, -0.39644498, -1.35441804,\n",
      "          0.14823900,  0.61021996,  0.22136298,  0.01457500, -0.51933700,\n",
      "         -0.03049497,  0.29500899,  1.21817601,  1.52853608, -0.29614100,\n",
      "         -0.22853000,  0.56629598,  0.18215299,  0.05563702, -0.14031701,\n",
      "          0.46038696, -0.39250997, -0.14935900, -0.18032800,  0.09984998,\n",
      "          0.82393301,  0.19287001,  1.30875206,  0.84635299, -0.08462198,\n",
      "         -0.87560296, -1.27083004, -0.56281102,  1.23273003, -0.12010300,\n",
      "          0.40309402,  0.09314601,  0.08564499,  0.52058601, -0.08435601,\n",
      "          0.20082700, -0.59016597, -0.14477898, -1.39974904,  0.89278197,\n",
      "         -0.18149300, -1.25894499,  0.34082299,  0.73809701, -0.13673499,\n",
      "          0.78250700,  0.05946300, -0.04962800,  0.13730299,  1.18410206,\n",
      "         -1.13477397,  0.01376298, -0.05336902, -1.60862803, -0.34568301,\n",
      "          1.01880789,  0.63131201,  0.05573400,  0.18241802,  0.10014699,\n",
      "          0.60922801, -0.30579099, -0.93282801,  0.74060404, -0.23849502,\n",
      "         -0.57471001,  0.13528301, -0.18460003, -0.86228204,  0.92488599,\n",
      "         -1.05108702,  0.57689404,  0.14059199, -0.67596495, -0.25736600,\n",
      "          0.13460998, -0.89346695,  0.13886604, -0.38684499, -1.01970291,\n",
      "          0.56374705,  0.88921201,  1.44379807,  0.80300295, -0.55586797,\n",
      "          0.09390201,  0.94733399, -0.06502296, -0.72694403, -0.82137799,\n",
      "         -0.28822300, -0.37720999, -0.56075400,  1.01989400,  0.26950496,\n",
      "          1.12077200, -0.21662501, -0.39902803, -0.94113904,  0.91748804,\n",
      "         -0.96163505,  0.56048602,  0.46968603, -0.27256101,  1.11441898,\n",
      "         -0.59233701,  0.85710895,  0.23405901, -0.75324297,  0.12230702,\n",
      "         -0.78117800, -0.10960899,  0.08155300,  1.06628406,  0.03035499,\n",
      "         -0.97897792,  0.31618202,  0.21204901, -0.13028100, -0.50344998,\n",
      "         -0.07627799, -0.49829501, -0.18986601, -0.64633000, -0.00865199,\n",
      "          0.23871900, -0.10368300, -0.32988700, -0.67868102, -0.75349200,\n",
      "          0.13453804, -0.20137599, -0.60196602, -0.58072203,  0.48392898,\n",
      "          0.05918600, -0.10954001, -0.36134800, -0.25066602, -0.48769000,\n",
      "          1.14627802, -0.04332298, -0.59548903, -0.06707900,  1.03566504,\n",
      "          0.28163600,  0.16956900, -0.18856801, -0.23614299,  0.55832601,\n",
      "          0.14457400,  0.78229600, -1.07994998, -0.75396705, -0.36890602,\n",
      "         -1.39206004,  0.02375700,  1.01109397, -0.19583800, -1.25814998,\n",
      "         -0.16565600, -0.21808805,  0.90738702, -0.21436599,  1.17099798,\n",
      "         -0.76871502,  0.43005404,  1.01214099,  0.09942500,  0.51968908,\n",
      "         -1.26677799, -1.33047605, -0.53693902,  0.80371296, -0.26477295,\n",
      "         -0.50434303,  0.74003005,  0.76988798, -0.50886798,  1.14536595,\n",
      "         -0.15646198,  0.03014898,  0.53830302, -0.92540193,  0.34569299,\n",
      "         -1.03690398, -0.86236000,  1.03571105, -1.16034400, -0.79153705,\n",
      "         -0.16215901, -0.14091200,  0.28341001, -0.29001799,  0.25445801,\n",
      "         -1.02832103,  0.73295200, -0.31887600,  0.61945796, -0.10724500,\n",
      "         -0.30411401,  0.70542002, -0.29804003, -0.31371796, -0.59208494,\n",
      "          0.35189199, -0.09597000, -0.10231099,  0.02775100,  0.12176699,\n",
      "         -0.45981401, -1.17983699, -0.85079604,  0.33981299, -0.34923100,\n",
      "         -1.31743908, -0.50696898,  0.47331199,  0.20623200,  0.55525202,\n",
      "         -0.09276600,  0.66634899, -0.00665502,  0.15876700,  0.83021605,\n",
      "          0.13043998,  0.40333200, -1.42592800,  0.06365600, -0.11724899,\n",
      "         -0.07423400,  0.13680205, -0.53506601, -0.25125903,  0.33414900,\n",
      "         -0.60567701,  0.60674995, -0.34135902, -0.42616796, -1.00366795,\n",
      "         -0.25675601,  0.03204599,  1.11357999, -1.11752903,  0.36861497,\n",
      "          0.98433203, -0.66891402, -0.84245402, -0.20175003, -0.09332900,\n",
      "          0.32493502, -0.19995201,  0.10618600,  0.95817697, -0.53943402,\n",
      "         -0.04115899,  0.33847195, -0.71650302,  0.08051599,  0.47381002,\n",
      "          0.45976400,  0.98668104, -0.13635202,  0.00737798,  0.55763298,\n",
      "         -0.77910805,  0.84435403, -0.75447106, -0.06560801, -0.57145703,\n",
      "         -0.11083099,  0.61097002,  0.11026701, -0.51492500, -0.01283800,\n",
      "         -0.17902598, -0.74405003, -1.73967910, -0.88179004,  1.21855092,\n",
      "         -0.57623899, -1.19014299,  0.01807701, -0.17290100, -0.29048300,\n",
      "          0.02186799,  0.16412997,  0.94055194,  0.70826697, -0.40343699,\n",
      "          0.24163100, -0.67855698, -0.09358700, -0.26306400, -0.88959199]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 JiebaTokenizer 进行分词\n",
    "from paddlenlp.data import JiebaTokenizer\n",
    "import paddlenlp\n",
    "\n",
    "vocab = token_embedding.vocab\n",
    "\n",
    "'''\n",
    "token_embedding 的核心内容就是一个 [635965, 300]的词表（vocab）；\n",
    "vocab 本质是一个 tocken_to_idx 的字典\n",
    "'''\n",
    "\n",
    "tokenizer = JiebaTokenizer(vocab) # 分词器使用提供的词表进行分词\n",
    "text = \"我来自清华大学\"\n",
    "tokens = tokenizer.cut(text)\n",
    "print(tokens)\n",
    "\n",
    "token_idxs = token_embedding.get_idx_list_from_words(tokens)\n",
    "embedding_tensors = token_embedding(paddle.to_tensor(token_idxs))\n",
    "print(embedding_tensors)\n",
    "\n",
    "# 使用词袋模型得到句向量\n",
    "bow_encoder = paddlenlp.seq2vec.BoWEncoder(token_embedding.embedding_dim)\n",
    "\n",
    "# 这里修改 shape 的原因是：BoWEncoder 的 forward 函数要求输入的格式是 (batch_size, num_tokens, emb_dim)\n",
    "# print(embedding_tensors.shape)\n",
    "paddle.unsqueeze_(embedding_tensors, axis=0)\n",
    "# print(embedding_tensors.shape)\n",
    "sentense_embedding = bow_encoder(embedding_tensors)\n",
    "print(sentense_embedding)\n",
    "# 至此我们得到了句向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attension 机制\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
