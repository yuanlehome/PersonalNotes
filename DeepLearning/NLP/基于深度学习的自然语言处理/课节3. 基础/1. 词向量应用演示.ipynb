{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddleNLPè¯å‘é‡åº”ç”¨å±•ç¤º\n",
    "\n",
    "6.7æ—¥NLPç›´æ’­æ‰“å¡è¯¾å¼€å§‹å•¦\n",
    "\n",
    "**[ç›´æ’­é“¾æ¥è¯·æˆ³è¿™é‡Œï¼Œæ¯æ™š20:00-21:30ğŸ‘ˆ](http://live.bilibili.com/21689802)**\n",
    "\n",
    "**[è¯¾ç¨‹åœ°å€è¯·æˆ³è¿™é‡ŒğŸ‘ˆ](https://aistudio.baidu.com/aistudio/course/introduce/24177)**\n",
    "\n",
    "æ¬¢è¿æ¥è¯¾ç¨‹**QQç¾¤**ï¼ˆç¾¤å·:618354318ï¼‰äº¤æµå§~~\n",
    "\n",
    "\n",
    "è¯å‘é‡ï¼ˆWord embeddingï¼‰ï¼Œå³æŠŠè¯è¯­è¡¨ç¤ºæˆå®æ•°å‘é‡ã€‚â€œå¥½â€çš„è¯å‘é‡èƒ½ä½“ç°è¯è¯­ç›´æ¥çš„ç›¸è¿‘å…³ç³»ã€‚è¯å‘é‡å·²ç»è¢«è¯æ˜å¯ä»¥æé«˜NLPä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚è¯­æ³•åˆ†æå’Œæƒ…æ„Ÿåˆ†æã€‚\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/54878855b1df42f9ab50b280d76906b1e0175f280b0f4a2193a542c72634a9bf\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p>\n",
    "<br><center>å›¾1ï¼šè¯å‘é‡ç¤ºæ„å›¾</center></br>\n",
    "\n",
    "PaddleNLPå·²é¢„ç½®å¤šä¸ªå…¬å¼€çš„é¢„è®­ç»ƒEmbeddingï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`paddlenlp.embeddings.TokenEmbedding`æ¥å£åŠ è½½å„ç§é¢„è®­ç»ƒEmbeddingã€‚æœ¬ç¯‡æ•™ç¨‹å°†ä»‹ç»`paddlenlp.embeddings.TokenEmbedding`çš„ä½¿ç”¨æ–¹æ³•ï¼Œè®¡ç®—è¯ä¸è¯ä¹‹é—´çš„è¯­ä¹‰è·ç¦»ï¼Œå¹¶ç»“åˆè¯è¢‹æ¨¡å‹è·å–å¥å­çš„è¯­ä¹‰è¡¨ç¤ºã€‚\n",
    "\n",
    "tokenï¼šæ¨¡å‹è¾“å…¥åŸºæœ¬å•å…ƒã€‚æ¯”å¦‚ä¸­æ–‡BERTä¸­ï¼Œtokenå¯ä»¥æ˜¯ä¸€ä¸ªå­—ï¼Œä¹Ÿå¯ä»¥æ˜¯<CLS>ç­‰æ ‡è¯†ç¬¦ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ token æ˜¯ word\n",
    "\n",
    "embeddingï¼šä¸€ä¸ªç”¨æ¥è¡¨ç¤ºtokençš„ç¨ å¯†çš„å‘é‡ã€‚tokenæœ¬èº«ä¸å¯è®¡ç®—ï¼Œéœ€è¦å°†å…¶æ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­å‘é‡ç©ºé—´ï¼Œæ‰å¯ä»¥è¿›è¡Œåç»­è¿ç®—ï¼Œè¿™ä¸ªæ˜ å°„çš„ç»“æœå°±æ˜¯è¯¥tokenå¯¹åº”çš„embeddingã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½TokenEmbedding\n",
    "\n",
    "`TokenEmbedding()`å‚æ•°\n",
    "- `embedding_name`\n",
    "å°†æ¨¡å‹åç§°ä»¥å‚æ•°å½¢å¼ä¼ å…¥TokenEmbeddingï¼ŒåŠ è½½å¯¹åº”çš„æ¨¡å‹ã€‚é»˜è®¤ä¸º`w2v.baidu_encyclopedia.target.word-word.dim300`çš„è¯å‘é‡ã€‚\n",
    "- `unknown_token`\n",
    "æœªçŸ¥tokençš„è¡¨ç¤ºï¼Œé»˜è®¤ä¸º[UNK]ã€‚\n",
    "- `unknown_token_vector`\n",
    "æœªçŸ¥tokençš„å‘é‡è¡¨ç¤ºï¼Œé»˜è®¤ç”Ÿæˆå’Œembeddingç»´æ•°ä¸€è‡´ï¼Œæ•°å€¼å‡å€¼ä¸º0çš„æ­£æ€åˆ†å¸ƒå‘é‡ã€‚\n",
    "- `extended_vocab_path`\n",
    "æ‰©å±•è¯æ±‡åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼Œè¯è¡¨æ ¼å¼ä¸ºä¸€è¡Œä¸€ä¸ªè¯ã€‚å¦‚å¼•å…¥æ‰©å±•è¯æ±‡åˆ—è¡¨ï¼Œtrainable=Trueã€‚\n",
    "- `trainable`\n",
    "Embeddingå±‚æ˜¯å¦å¯è¢«è®­ç»ƒã€‚Trueè¡¨ç¤ºEmbeddingå¯ä»¥æ›´æ–°å‚æ•°ï¼ŒFalseä¸ºä¸å¯æ›´æ–°ã€‚é»˜è®¤ä¸ºTrueã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "\n",
    "# åˆå§‹åŒ–TokenEmbeddingï¼Œ é¢„è®­ç»ƒembeddingæœªä¸‹è½½æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æ•°æ®\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")\n",
    "\n",
    "# æŸ¥çœ‹token_embeddingè¯¦æƒ…\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®¤è¯†ä¸€ä¸‹Embedding\n",
    "**`TokenEmbedding.search()`**\n",
    "è·å¾—æŒ‡å®šè¯æ±‡çš„è¯å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_embedding = token_embedding.search(\"ä¸­å›½\")\n",
    "print(type(test_token_embedding))\n",
    "import paddle\n",
    "test_token_embedding_tensor = paddle.to_tensor(test_token_embedding)\n",
    "print(test_token_embedding_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`TokenEmbedding.cosine_sim()`**\n",
    "è®¡ç®—è¯å‘é‡é—´ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¯­ä¹‰ç›¸è¿‘çš„è¯è¯­ä½™å¼¦ç›¸ä¼¼åº¦æ›´é«˜ï¼Œè¯´æ˜é¢„è®­ç»ƒå¥½çš„è¯å‘é‡ç©ºé—´æœ‰å¾ˆå¥½çš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = token_embedding.cosine_sim(\"å¥³å­©\", \"å¥³äºº\")\n",
    "score2 = token_embedding.cosine_sim(\"å¥³å­©\", \"ä¹¦ç±\")\n",
    "print('score1:', score1)\n",
    "print('score2:', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºäºTokenEmbeddingè¡¡é‡å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦\n",
    "\n",
    "åœ¨è®¸å¤šå®é™…åº”ç”¨åœºæ™¯ï¼ˆå¦‚æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿï¼‰ä¸­ï¼Œ éœ€è¦è¡¡é‡ä¸¤ä¸ªå¥å­çš„è¯­ä¹‰ç›¸ä¼¼ç¨‹åº¦ã€‚æ­¤æ—¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼Œç®€ç§°BoWï¼‰è®¡ç®—å¥å­çš„è¯­ä¹‰å‘é‡ã€‚\n",
    "\n",
    "**é¦–å…ˆ**ï¼Œå°†ä¸¤ä¸ªå¥å­åˆ†åˆ«è¿›è¡Œåˆ‡è¯ï¼Œå¹¶åœ¨TokenEmbeddingä¸­æŸ¥æ‰¾ç›¸åº”çš„å•è¯è¯å‘é‡ï¼ˆword embddingï¼‰ã€‚\n",
    "\n",
    "**ç„¶å**ï¼Œæ ¹æ®è¯è¢‹æ¨¡å‹ï¼Œå°†å¥å­çš„word embeddingå åŠ ä½œä¸ºå¥å­å‘é‡ï¼ˆsentence embeddingï¼‰ã€‚\n",
    "\n",
    "**æœ€å**ï¼Œè®¡ç®—ä¸¤ä¸ªå¥å­å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚\n",
    "\n",
    "### åŸºäºTokenEmbeddingçš„è¯è¢‹æ¨¡å‹\n",
    "\n",
    "\n",
    "ä½¿ç”¨`BoWEncoder`æ­å»ºä¸€ä¸ªBoWæ¨¡å‹ç”¨äºè®¡ç®—å¥å­è¯­ä¹‰ã€‚\n",
    "\n",
    "* `paddlenlp.TokenEmbedding`ç»„å»ºword-embeddingå±‚\n",
    "* `paddlenlp.seq2vec.BoWEncoder`ç»„å»ºå¥å­å»ºæ¨¡å±‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddlenlp\n",
    "\n",
    "\n",
    "class BoWModel(nn.Layer):\n",
    "    def __init__(self, embedder):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        emb_dim = self.embedder.embedding_dim\n",
    "        self.encoder = paddlenlp.seq2vec.BoWEncoder(emb_dim)\n",
    "        self.cos_sim_func = nn.CosineSimilarity(axis=-1)\n",
    "\n",
    "    def get_cos_sim(self, text_a, text_b):\n",
    "        text_a_embedding = self.forward(text_a)\n",
    "        text_b_embedding = self.forward(text_b)\n",
    "        cos_sim = self.cos_sim_func(text_a_embedding, text_b_embedding)\n",
    "        return cos_sim\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, embedding_dim)\n",
    "        summed = self.encoder(embedded_text)\n",
    "        return summed\n",
    "\n",
    "model = BoWModel(embedder=token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ„é€ Tokenizer\n",
    "ä½¿ç”¨TokenEmbeddingè¯è¡¨æ„é€ Tokenizerã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "print(token_embedding)\n",
    "\n",
    "tokenizer.set_vocab(vocab=token_embedding.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç›¸ä¼¼å¥å¯¹æ•°æ®è¯»å–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = {}\n",
    "with open(\"text_pair.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        text_a, text_b = line.strip().split(\"\\t\")\n",
    "        if text_a not in text_pairs:\n",
    "            text_pairs[text_a] = []\n",
    "        text_pairs[text_a].append(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹ç›¸ä¼¼è¯­å¥ç›¸å…³åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_a, text_b_list in text_pairs.items():\n",
    "    text_a_ids = paddle.to_tensor([tokenizer.text_to_ids(text_a)])\n",
    "    embedded_text = token_embedding(text_a_ids)\n",
    "\n",
    "    for text_b in text_b_list:\n",
    "        text_b_ids = paddle.to_tensor([tokenizer.text_to_ids(text_b)])\n",
    "        print(\"text_a: {}\".format(text_a))\n",
    "        print(\"text_b: {}\".format(text_b))\n",
    "        print(\"cosine_sim: {}\".format(model.get_cos_sim(text_a_ids, text_b_ids).numpy()[0]))\n",
    "        print()\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
