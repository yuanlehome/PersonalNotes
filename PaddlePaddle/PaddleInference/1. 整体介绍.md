paddle inferece 是一个针对 paddle 模型的推理引擎， 支持多硬件后端。本文是对 inference 的一个简单介绍以及 API 使用。

更多内容可以查看[官方文档](https://www.paddlepaddle.org.cn/inference/product_introduction/inference_intro.html)

以下所有内容基于 paddle inference [release/2.4](https://github.com/PaddlePaddle/Paddle/tree/release/2.4) 分支， 且以 GPU 后端为例。

`inference 编译指令`

```bash

# cpu(mkldnn, mkl)
cmake ..  -DCMAKE_BUILD_TYPE=Release -DON_INFER=ON  -DPY_VERSION=3.7  -DWITH_MKL=ON  -DWITH_MKLDNN=ON  -DWITH_GPU=OFF

# gpu(cudnn) 
cmake ..  -DCMAKE_BUILD_TYPE=Release -DON_INFER=ON  -DPY_VERSION=3.7 -DWITH_GPU=ON  -DCUDA_ARCH_NAME=Auto  -DWITH_TENSORRT=OFF \-DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ -DWITH_INFERENCE_NVTX=ON

# cpu(mkldnn, mkl) + gpu(cudnn) + tensorrt 
cmake ..  -DCMAKE_BUILD_TYPE=Release -DON_INFER=ON  -DPY_VERSION=3.7  -DWITH_MKL=ON  -DWITH_MKLDNN=ON  -DWITH_GPU=ON  -DCUDA_ARCH_NAME=Auto -DWITH_TENSORRT=ON -DTENSORRT_ROOT=/weishengying/download/TensorRT-8.4.1.5 -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/

# gpu with tensorRT, cudnn
cmake ..  -DPY_VERSION=3.7  -DWITH_TENSORRT=ON  -DON_INFER=ON  -DCUDA_ARCH_NAME=Auto   -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ -DTENSORRT_ROOT=/weishengying/download/TensorRT-8.4.1.5 -DWITH_TESTING=OFF -DWITH_INFERENCE_NVTX=ON

cmake ..  -DPY_VERSION=3.7  -DWITH_MKL=ON  -DWITH_MKLDNN=ON  -DWITH_TENSORRT=ON  -DON_INFER=ON  -DCUDA_ARCH_NAME=Auto   -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ -DCUDNN_ROOT=/weishengying/download/cudnn-linux-x86_64-8.6.0.163_cuda11-archive -DTENSORRT_ROOT=/weishengying/download/TensorRT-8.4.1.5 -DWITH_TESTING=OFF -DWITH_INFERENCE_NVTX=ON

# with gpu, cudnn
cmake ..  -DPY_VERSION=3.7  -DWITH_MKL=ON  -DWITH_MKLDNN=ON  -DON_INFER=ON  -DCUDA_ARCH_NAME=Auto   -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ -DCUDNN_ROOT=/weishengying/download/cudnn-linux-x86_64-8.6.0.163_cuda11-archive -DWITH_TESTING=OFF -DWITH_INFERENCE_NVTX=ON

cmake .. -DPY_VERSION=3.7 -DWITH_GPU=ON -DWITH_TESTING=OFF -DCMAKE_BUILD_TYPE=Release -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/  -DWITH_INFERENCE_NVTX=ON

cmake .. -DPY_VERSION=3.7 -DWITH_GPU=ON -DWITH_TESTING=OFF -DCMAKE_BUILD_TYPE=Release -DWITH_DISTRIBUTE=ON  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/  -DWITH_INFERENCE_NVTX=ON
```

# 1. 预测示例 C++

具体步骤可以查看官方文档，这里直接提供完整代码（以 resnet-50 模型为例, 下载方式 wget https://paddle-inference-dist.bj.bcebos.com/Paddle-Inference-Demo/resnet50.tgz）。 [完整代码](./code_resNet_50/main.cc)

预测代码只需要引用头文件 `#include "paddle/include/paddle_inference_api.h"`

一起研究下， paddle-inference 的 API 体系。

# 2. paddle-inference 的 API 体系

最外层的接口是 `paddle_infer::Predictor`,它是提供给用户的类，接口也很简单，意义很明确。`paddle_infer::Predictor`内部封装的是 `paddle::PaddlePredictor`的指针，这是一个基类，它的派生类是 `paddle::AnalysisPredictor` 和 `paddle::NativePaddlePredictor`。

在推理的过程中，只使用的 `paddle::AnalysisPredictor`，因为该预测器会对模型进行一些 `分析优化`。AnalysisPredictor 有一些重要的成员：

```cpp
AnalysisConfig config_;
Argument argument_;
std::unique_ptr<NaiveExecutor> executor_;
platform::Place place_;
std::shared_ptr<framework::Scope> scope_;
framework::Scope *sub_scope_{nullptr};
std::shared_ptr<framework::ProgramDesc> inference_program_;
```

现在来研究 `AnalysisPredictor`,研究 paddle_inference 模型推理的各个阶段的细节问题。

# 3. paddle inference 推理流程

## 3.1 首先调用 `CreatePaddlePredictor`,根据传入的 `AnalysisConfig`创建预测器，并初始化预测器。

```shell
template <>
std::unique_ptr<PaddlePredictor>
CreatePaddlePredictor<AnalysisConfig, PaddleEngineKind::kAnalysis>(const AnalysisConfig &config)
```

## 3.2 初始化流程

### 3.2.1 PrepareScope

scope的本质的是一个哈希容器，用来管理推理过程中的变量。`std::unordered_map<std::string, std::unique_ptr<Variable>>` key是变量名， value是变量指针
PrepareScope 过程中会创建两个 scpoe，一个父scope（scope_），一个子scope（sub_scope_）。

### 3.2.2 CreateExecutor

创建一个 NaiveExecutor 执行器, 这是一个 `串行`执行器，根据 一个特定的 op 顺序逐个执行。所以 NaiveExecutor 里面的主要成员就是 `算子（op）` 和 `变量（variable）`。变量使用 `scope` 管理

```cpp
const platform::Place place_;
// Catch the required resource to avoid recreate.
std::vector<std::unique_ptr<OperatorBase>> ops_;
Scope* scope_;`
```

### 3.2.3 PrepareProgram

正如这一步的名字，这一步主要作用是 `得到优化后的模型`。
里面调用了 `OptimizeInferenceProgram()`,该 API 会跑一些 `AnalysisPass`。

`注意点1`

paddle的模型表示与onnx最大的区别是 paddle 模型用两个文件表示， xxx.pdmodel 表示模型的拓扑结构， xxx.pdiparams 表示模型的参数。
推理时，参数也是 op 的输入，op 的输入有两种，一种是上一个 op 的输出，这是个变量，另外一个是 op 的权重（参数），不管是哪一种，在 paddle 内部中都用
variable 表示，并用 persistable variable 和 variable 这种称呼区别 op 不同输入类型。 所有的variable 又是集中放在 scope 中管理的。所以用两个 scope 分别管理，就是上面所有的 父scope 和 子scope， 父scope 管理 persistable variable， 子scope 管理 variable。

上述部分的实现对应源码是 `executor_->CreateVariables` api, 如

```cpp
executor_->CreateVariables(*inference_program_, 0, true, sub_scope_);
```

吐槽一句，这个接口的调用和实现”有点怪“，简单的功能实现得有点绕圈。但是功能和上述描述是吻合的。
`同时，PrepareProgram 中只创建 主block 中的所有变量，而 子block 中的变量不被创建。子block对应一些控制流Op，里面的变量在控制流执行的时候被创建，后面会做详细的介绍`

`注意点2`

CreateVariables 接口只是在 scope 中创建一个变量，然后根据变量的类型初始化这个变量，比如变量类型是 LOD_Tensor, 就在变量中创建一个空LOD_Tensor。
变量 variable 可以理解 std::any(c++17), 它可以赋值为多种不同类型。对于op输入的权重对应的变量， 还需要把权重的具体数据，填充到对应变量中的 LOD_Tnsor 中，这个过程发生在 `ir_graph_build_pass` 中

`注意点3`

经过这一步骤之后，得到的是优化之后的模型，即 AnalysisPredictor的成员 `inference_program_`。

### 3.2.4 PrepareFeedFetch

严格来说，这一个步骤是多余的，因为当前paddle inference 并不跑 feed， fetch 这些 Op（这样可以提高模型推理速度，减少一个op的调度，onnx的模型表示中也没有feed fetch op），虽然模型的拓扑结构中有表示。但是由于inference 内部实现有一点混乱，直接在  `AnalysisPredictor::Init`中注释掉 `PrepareFeedFetch`这个API的调用会报错，但是注释掉 `PrepareFeedFetch`中的 `CreateFeedFetchVar`的调用是可以的，即在 scope 中不创建"feed","feed"这些变量是合理的。

### 3.2.5 PrepareExecutor

该步骤就是根据优化后的 inference_program_，在 NaiveExecutor 中创建所有的 Op。

# 4. paddle inference 模型优化流程介绍

## 4.1 paddle inference 中的 pass

inference 中的 pass 可以认为分为两类，一类可以认为是与硬件无关的，称为 `AnalysisPass`， 源码中的说明是：
`AnalysisPass is a pass used to control the IR passes`。 这类pass 基本并不做模型层面的优化，有一些pass是推理过程中必须跑的。

`AnalysisPass` 中有一个特殊的 pass：`ir_analysis_pass`。这个 `ir_analysis_pass`中包含了跟硬件相关的图形层面的优化的 `IRPass`, 如果不跑这个 ir_analysis_pass，就不会做模型层面上的优化。

所有的 `AnalysisPass`都在目录 `paddle/fluid/inference/analysis/passes`下：

这些 AnalysisPass 都提前注册在了 `PassRegistry`中（paddle/fluid/inference/analysis/passes/passes.h）,可以根据 pass 的名字找到对应的pass类。

```cpp
PassRegistry::PassRegistry() {
  // Register manually to avoid the trivial `USE_OP` like macro for easier use
  // and link.
  passes_.emplace("ir_analysis_pass",
                  std::unique_ptr<AnalysisPass>(new IrAnalysisPass));
  passes_.emplace("ir_graph_build_pass",
                  std::unique_ptr<AnalysisPass>(new IrGraphBuildPass));
  passes_.emplace("ir_graph_clean_pass",
                  std::unique_ptr<AnalysisPass>(new IrInferCleanGraphPass));
  passes_.emplace("memory_optimize_pass",
                  std::unique_ptr<AnalysisPass>(new MemoryOptimizePass));
  passes_.emplace(
      "ir_params_sync_among_devices_pass",
      std::unique_ptr<AnalysisPass>(new IrParamsSyncAmongDevicesPass));
  passes_.emplace("adjust_cudnn_workspace_size_pass",
                  std::unique_ptr<AnalysisPass>(new AdjustCudnnWorkSpacePass));
  passes_.emplace("inference_op_replace_pass",
                  std::unique_ptr<AnalysisPass>(new InferenceOpReplacePass));
  passes_.emplace(
      "ir_graph_to_program_pass",
      std::unique_ptr<IrGraphToProgramPass>(new IrGraphToProgramPass));
}
```

推理过程中，不同硬件跑哪些 pass 由 `PaddlePassBuilder`这个类及其派生类控制（paddle/fluid/inference/api/paddle_pass_builder.h，
PaddlePassBuilder 中的成员 analysis_passes_， 里面的 pass 是推理过程中必须要跑的 pass，

```cpp
std::vector<std::string> analysis_passes_{
{"ir_graph_build_pass",
  "ir_graph_clean_pass",
  "ir_analysis_pass",
  "ir_params_sync_among_devices_pass",
  "adjust_cudnn_workspace_size_pass",
  "inference_op_replace_pass"}};
```

PaddlePassBuilder 的不同派生类的 pass，都是一些 `ir_analysis_pass`，这些 pass 是根据不同的硬件平台，对模型做一些优化，称作 `PassStrategy`, 有 `CpuPassStrategy`, `GpuPassStrategy`等等。

`注意点1`

AnalysisPredictor 中的成员 `argument_`,是一个非常重要的数据结构，在 `AnalysisPredictor::PrepareArgument()`接口中，将推理过程中需要跑哪些 pass 的名字全部都记录下，通过api（`argument_.SetIrAnalysisPasses, argument_.SetAnalysisPasses`)

`注意点2`

最要的一个 pass 必须是 `ir_graph_to_program_pass`，得到的是优化之后的模型，即 AnalysisPredictor的成员 `inference_program_`。

```cpp
PaddlePassBuilder::AnalysisPasses(){
  auto passes = analysis_passes_;
  // To make sure the ir_graph_to_program should be the last pass so any”
  // modication of IR will persist to the program.
  passes.push_back("ir_graph_to_program_pass");
  return passes;
}
```

后续逐个研究这些推理过程中必须要跑的 AnalysisPass

# 5 ir_graph_build_pass (paddle inference 的 IR 设计)

这是第一个跑的 pass， 将模型 program， 转换为一个中间表示（IR），称为 graph,方便后面的 pass 做处理。

inference 的 IR 是用一个叫做 graph 的数据结构表示，有一个 main_graph，它相当于一个容器，里面存子 graph， program 中的每一个 block 对应一个子 graph。
如下图所示，假设 program 中有三个子 block， 则一个可能的对应的 graph 的结构如下所示：

```cpp
     main_graph(main_graph_(nullptr))
    /               |              \
   /                |               \
  /                 |                \
sub_graph        sub_graph          sub_graph 
(id: 0           (id: 1             (id: 2
parent_id: -1)   parent_id: 0)      parent_id: 0)
```

`由于 paddle 模型的特殊性，导致 paddle 的 program 模型并不是一个 DAG，一般的模型（如 onnx），除了输入变量以及权重变量，其他中间变量都是 ”read after write“(先写后读)， 且中间变量只会被一个 Op 写入，即模型表示满足SSA。`

paddle 的 program 模型并不满足 SSA 形式，所以 inference 框架内部将 program 转换为 IR 的过程中，做了一些特殊处理，使得 IR 满足 SSA 形式，IR 也是一个 DAG。

paddle 的 program 模型的中间变量存在一些 ”write after read(WAR) and write after write(WAW)“ 的场景， WAW 很好理解， WAR 可能让有些人感到困惑， paddle 内部有一个特殊的变量——`vecotr<tensor>`，可以首先通过 `LodarrayLength`这个op 获得 vecotr 的长度（0），然后在 vector 下标 0 的位置 写入一个 tensor。这样对于 vecotr`<tensor>` 这种变量来说，就是先读后写。

`吐槽一句`

在 `graph::InitFromBlock` 函数中有这么一段话：

```cpp
PADDLE_ENFORCE_EQ(out_arg_set.count(each_var_name),
                          0,
                          platform::errors::InvalidArgument(
                              "The input Program is invalid. Variable %s occurs"
                              " in output of %s multiple times.",
                              each_var_name,
                              op->Type()));
```

给人的感觉就是如果模型中存在一个变量被多次写入，这个模型就是不合法的，会报错。但是其实根本不是，仔细阅读这个函数的实现就会发现：这部分只是检测一个Op中不允许出现两个同名输出变量。所以这段检测代码非常无用，而且容易给阅读者带来误解。

对于这种不符合 SSA 形式的模型，inference的处理如下所示：`对应函数 Graph::ResolveHazard()`
假设一个 write_after_write 可能的模型组网描述如下：

```python
b = op1(a)
b = op2(c)
d = op3(b)
```

则对应的 IR 如下：c_var = __control_var

```cpp
  node(a)               node(c)
    |                     |
    |                     |
  node(op1) -->c_var--> node(op2)
    |                     |
    |                     |
  node(b)               node(b')
                          |
                          |
                        node(op3)
                          |
                          |
                        node(d)
```

假设一个 write_after_write 可能的模型组网描述如下：

```python
b = op1(a)
c = op2(b)
b = op3(d)
e = op4(b)
```

则对应的 IR 如下：

```cpp
  node(a)               node(d)
    |                     |
    |                     |
  node(op1)--> c_var--> node(op3)
    |                  ^  |
    |                 /   |
  node(b)       c_var   node(b')
    |         /           |
    |       /             |
  node(op2)             node(op4)
    |                     |
    |                     |
  node(c)               node(e)
```

假设一个 read_after_write 可能的模型组网描述如下：

```python
b = op1(a)
a = op2(c)
```

则对应的 IR 如下：

```cpp
  node(a)               node(c)
    |                     |
    |                     |
  node(op1) -->c_var--> node(op2)
    |                     |
    |                     |
  node(b)               node(a')
```

这可能是一种保证正确执行顺序的处理方式，但是引入了一些所谓的 contol_var 来保证算子之间的依赖关系，这会使得某些 op 的输入或者输出多一个变量，影响 pass 中的匹配逻辑。
其实有更好的方式来保证得到正确的执行顺序。（得到执行顺序并非只能依靠拓扑排序，原先的program中是有执行顺序的）

# 6 ir_graph_clean_pass

前面 5 中的内容吐槽了通过增加 control_var 这种方式来保证拓扑排序的鸡肋，`ir_graph_clean_pass`的逻辑很简单，目的就是为了去掉 `ir_graph_build_pass`中构建 IR 时增加的 control_var 变量，一加一减，白折腾了。
所以这个 pass 和 `ir_graph_build_pass`中的 `graph::ResolveHazard()`api都可以删掉。

`注：上述吐槽的这些 IR 的鸡肋问题，inference后续都修改删除了。` [remove all control_vars in IR graph](https://github.com/PaddlePaddle/Paddle/pull/46888)

# 7 ir_analysis_pass

该 pass 会做一些模型层面上的优化，是非必要的pass，后续在做详细的解释。

# 8 ir_params_sync_among_devices_pass

该 pass 将 cpu 上储存的权重数据拷贝到 GPU。

# 9 adjust_cudnn_workspace_size_pass and inference_op_replace_pass

这两个 pass 逻辑简单，不做多余叙述。

# 10 ir_graph_to_program_pass

这是最后一个 pass，将最终优化后的 IR 转换为执行 program， 对应 `AnalysisPredictor::inference_program_`成员对象。内部的主要逻辑是对 IR 进行拓扑排序，得到最终执行的 program。
